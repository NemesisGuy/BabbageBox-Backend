# âš™ï¸ BabbageBox Backend

> **The high-performance core of BabbageBox. Orchestrates GGUF inference, RAG memory, and multi-modal tool calling.**

---

## âœ¨ Key Features

- **ðŸ§© Modular Chat Harness**: Centralized prompt templates in `app/chat/` for precise control over TinyLlama, Gemma, and Qwen models.
- **ðŸš€ Agile Inference**: Optimized `llama-cpp-python` integration with auto-detecting performance profiles.
- **ðŸ§  Vector Memory**: Long-term contextual memory using SQLite and FAISS.
- **ðŸŽ™ Supertonic TTS**: Native integration for high-fidelity speech synthesis.
- **ðŸ” MCP Search**: Autonomous web search capabilities via DuckDuckGo and Wikipedia.

---

## ðŸ›  Installation

### 1. Environment Setup

```bash
python -m venv .venv
source .venv/bin/activate  # On macOS/Linux
# OR
.venv\Scripts\activate     # On Windows
```

### 2. Dependencies

```bash
pip install -r requirements.txt
```

---

## ðŸƒ Running the Engine

Start the FastAPI worker using [uvicorn](https://www.uvicorn.org/):

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

> [!TIP]
> Use the `--reload` flag during development for hot-reloading on code changes.

---

## ðŸ§ª Testing & Verification

Comprehensive test suite covering prompt fidelity and API integrity:

```bash
pytest tests/test_api.py
pytest tests/test_gemma_harness.py
pytest tests/test_tinyllama_harness.py
```

---

## ðŸ“ System Folders

- `/app/chat`: Unified location for model configurations and prompt templates.
- `/data`: SQLite database (`babbage.db`) and FAISS indexes.
- `/models`: (Symlinked/Root) GGUF model storage.

---

> [!IMPORTANT]
> This backend is designed for local-first privacy. Ensure your `LLAMA_MODEL_PATH` points to a valid GGUF file for inference.



 cd "C:\Users\Reign\Documents\Python Projects\BabbageBox"; & ".\.venv\Scripts\Activate.ps1"; cd Babbagebox-Backend; python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reloadb